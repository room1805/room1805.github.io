---
layout: distill
permalink: /room1807/
title: 'korean'
post_title: 'Explore Input Specific Refusal Directions in Large Language Models'
description: 'Refusal directions are not general, but rather input specific. This work discover the effective refusal directions for the clustered inputs to broad the understanding on the interpretation of refusal directions in LLMs' 
tags: distill
giscus_comments: false
date: 2024-07-04
featured: true
nav: false
nav_order: 1
pagination:
  enabled: false
authors:
  - name: Bumjin Park
    affiliations:
      name: KAIST
bibliography: all.bib
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

## Introduction 

In recent studies, we found some interesting properties of refusal mechanisms in LLMs. 

1. LLMs tend to refusal more on the complete sentences, while showing jailbreaks on incomplete prompts [room 1805](https://room1805.github.io/room1805/). 
2. Refusal directions are input-specific and there could be several refusal directions for refusal types. [room 1806](https://room1805.github.io/room1806/) 

Accordingly, we are interested in finding **more finite refusal directions** that can explain the refusal mechanisms in LLMs. The finite refusal directions comes from the assumption that

> There exist countable refusal directions that controls the generation of refusal phrases in LLMs.

For this purpose, we propose a novel method to cluster sample-specific refusal directions based on the similarity of the most effective refusal directions. 



## Methods 

Refusal Specific Clustering Methods

Consider prompts $\mathcal{P}$ which includes both harmful and harmless prompts. 
As noted by the previous jailbreak works, prompts highly effect jailbreaks of LLMs. 
Although there could be various types of prompts, this work assumes that LLMs trained with RLHF knows the inputs patterns which are expected to be refused to answers. 

Clarifying the prompt types can further motivate the LLM developers to train more robust refusal mechanisms. 

> One might argue that revealing the prompt types might motivate more jailbreaks of LLMs. However, this does not happen when the inner representations are not provided to the end users, Which is currently out of scopes. 

This work follows the linear representation hypothesis in previous interpretation studies in LLMs <d-cite key="templeton2024scaling"> </d-cite>.   


### Finding Refusal Directions

### Option 1

$$
\begin{gather}
h_{i+1}^{point} = h_i + \nabla f(Y) \\  
h_{i+1}^{anchor}  = h_i - \nabla f(Y)
\end{gather}
$$

The first one find the refusal point while the second one find the anchor point. 

### Option 2

Anther option can be obtained by following the previous work on concept vectors. 

We additionally normalize the vectors. 
The benefit is the robustness of anchor point for the refusal directions. However, setting the anchor points can be biased. 


### Clustering Refusal Directions

## Experiments

## Datasets

Data plays a crucial role to find the refusal directions. As we find the direction in a contrastive manner for two data points, it is important to determine the anchor point. 

* Representation point : the last representation of a prompt in a specific layer.  
* Anchor point: th



Given a representation $h$, we find the refusal and harmful representations by running the activation maximization technique. 

